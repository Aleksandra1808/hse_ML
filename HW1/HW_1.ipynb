{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1: линейная регрессия и векторное дифференцирование (10 баллов).\n",
    "\n",
    "* Максимальное количество баллов за задания в ноутбуке - 11, но больше 10 оценка не ставится, поэтому для получения максимальной оценки можно сделать не все задания.\n",
    "\n",
    "* Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмите остаток от деления на 4 и прибавьте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим многомерную регрессию из sklearn для стандартного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1055636784402496e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.443560844436639e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.35266327e-08,  4.94259328e-08,  1.12400153e-08, -3.05733483e-08,\n",
       "        7.18974998e-08,  2.42668001e-08,  3.09321356e-08, -3.94059645e-08,\n",
       "        4.59969785e-08,  1.23099967e-08,  5.28065964e+01, -2.21884940e-08,\n",
       "        5.66808860e-08,  4.64727170e+01,  2.16010837e-08,  4.54775902e+01,\n",
       "       -1.42990963e-08, -4.29687043e-09,  9.97240678e-08, -3.29325771e-08,\n",
       "        7.06267285e-09,  2.07641323e-08,  1.04549255e-08, -3.51384305e-08,\n",
       "       -2.99822476e-08, -5.64887254e-08, -3.96054708e-08, -5.49037642e-08,\n",
       "       -1.82073971e-08, -2.42343873e-08,  6.36830954e+01, -3.07315805e-08,\n",
       "        3.77343553e-08, -9.77225318e-09, -1.84412945e-08, -1.89080760e-08,\n",
       "       -1.39550484e-08, -2.42122461e-08, -8.84695874e-09,  3.78158778e-08,\n",
       "        2.64379685e-08, -2.27179620e-08, -5.42509897e-08,  2.57857978e+01,\n",
       "       -1.19345604e-09, -2.71821932e-08, -5.01083560e-08,  2.52855525e-08,\n",
       "        5.00525152e-09,  9.89650074e-09, -5.59525641e-09, -2.46061519e-08,\n",
       "       -2.60418457e-08, -7.24586379e-08, -4.28631617e-08,  1.44856343e-08,\n",
       "       -2.41452342e-08,  1.37069719e-08,  1.41748059e-08, -2.85602892e-09,\n",
       "       -2.60099927e-08, -1.43058725e-08,  1.26314159e-08, -3.13231798e-08,\n",
       "        2.37876791e-08,  9.52773863e-08,  6.44298771e+01, -8.32387449e-08,\n",
       "       -4.08747330e-08, -7.91680962e-08, -8.62549063e-10,  9.62751751e+01,\n",
       "       -3.85043274e-08, -1.13812551e-07, -2.39096198e-08, -3.26047666e-09,\n",
       "        1.33413513e-09,  7.64753592e-08,  4.35564481e-08,  6.87780839e+01,\n",
       "        3.04122325e-08,  8.18009386e+01, -1.91813945e-08,  4.05381241e-09,\n",
       "       -5.59544162e-08,  4.41802728e-08,  3.17521398e-08,  1.82847330e-08,\n",
       "        1.09097639e-08,  3.23609961e-08,  5.85486312e-08,  8.61605316e+01,\n",
       "        5.19208773e-08,  2.44199304e-08,  5.51079180e-08,  8.00417801e-08,\n",
       "        1.51470852e-09, -1.61946249e-08,  6.25861705e-08, -7.72814911e-08])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 1 (0.5 балла).*** Объясните, чем вызвано различие двух полученных значений метрики?\n",
    "\n",
    "***Задание 2 (0.5 балла).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ответ к Заданию 1:***\n",
    "Дело в том, что LinearRegression решает задачу аналитически, то есть в явном виде подставляем в формулу значения. В свою очередь SGDRegressor - это итеративный метод, то есть задача решается численно. Кроме этого, в SGDRegressor встроена регурялизация L2, которая помогает избавиться от переобучения, из-за этого увеличивается значение MSE.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ответ к Заданию 2:***\n",
    "Для приближении MSE в методе градиентного спуска к MSE в LinearRegression я выбрала следующие гиперпараметры: alpha=0.000031, max_iter = 2100, n_iter_no_change = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1253174373659446e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4.88368287e-05,  1.32432795e-04,  1.64733651e-04,  2.61234288e-05,\n",
       "        9.56545530e-05,  2.04309111e-05,  6.35014770e-05, -1.96422084e-04,\n",
       "        9.83813690e-05,  2.14981012e-05,  5.28050199e+01,  1.76469632e-05,\n",
       "        7.26647266e-05,  4.64713724e+01,  3.33721656e-05,  4.54761436e+01,\n",
       "        3.26718414e-05,  3.20711351e-05, -2.03714847e-05,  1.46703517e-04,\n",
       "        4.45838208e-05, -1.46032302e-05,  1.10360126e-04,  8.78464863e-06,\n",
       "       -5.47519459e-05, -3.97071951e-05,  1.21939721e-04, -1.53842933e-04,\n",
       "       -7.46579434e-06,  1.53807530e-04,  6.36813092e+01, -1.85993888e-04,\n",
       "        7.68077499e-05,  1.39062482e-04,  1.28651683e-04,  1.72340543e-04,\n",
       "       -6.52811336e-05,  4.58258889e-05, -1.74274649e-04, -8.39566962e-05,\n",
       "        1.27637209e-04, -1.46128910e-04, -1.93574516e-04,  2.57850867e+01,\n",
       "       -1.11780823e-04, -2.27374016e-04,  1.13892307e-04,  4.24682103e-06,\n",
       "       -3.41358545e-05,  7.33803426e-05, -2.48799681e-05,  8.69386066e-05,\n",
       "       -1.11867100e-04, -2.30150717e-04, -3.33835747e-05, -1.32987472e-05,\n",
       "        5.11721074e-05,  1.32237867e-04,  2.95390757e-05,  1.25033744e-05,\n",
       "       -1.78176703e-04,  2.47271045e-06,  1.54714367e-04, -1.75674417e-04,\n",
       "        1.31814796e-04, -1.68491156e-05,  6.44280521e+01, -1.20373329e-04,\n",
       "       -5.80756440e-05, -2.85357192e-05,  1.69218611e-05,  9.62721785e+01,\n",
       "        2.05671984e-04, -1.00826696e-04,  1.50873233e-04,  1.34670222e-04,\n",
       "       -6.16974092e-05,  2.64836795e-04, -1.82719632e-04,  6.87759871e+01,\n",
       "        1.15108195e-04,  8.17985304e+01, -1.04135608e-04, -9.38655723e-05,\n",
       "        2.64867662e-04, -7.90048658e-05,  7.74949579e-05, -2.78521190e-04,\n",
       "       -1.68814891e-04, -2.93668120e-04, -1.40693171e-04,  8.61577402e+01,\n",
       "       -3.31051036e-05,  7.63475247e-05,  2.26694436e-04,  3.75586427e-05,\n",
       "        7.32875548e-05,  1.54909137e-04,  1.84410837e-05, -6.83458583e-05])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.000031, max_iter = 2100, n_iter_no_change = 10).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс. \n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и второй варианты) или l2 (третий и четвертый варианты) регуляризацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, alpha=0.1, l_ratio=0.000001, tol=0.0001, max_iter=1000):\n",
    "        '''\n",
    "        Для начала необходимо инициализировать параметры\n",
    "        alpha - это learning rate или шаг обучения\n",
    "        l_ratio - параметр регуляризации\n",
    "        tol - значение для критерия останова\n",
    "        max_iter - максимальное количество итераций обучения\n",
    "        '''\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "             \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Метод для обучения линейной регрессии\n",
    "        X - матрица признаков\n",
    "        y - вектор правильных ответов\n",
    "        '''\n",
    "        \n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        W = np.zeros(X.shape[1])\n",
    "        N = X.shape[0]\n",
    "        y_pred = X @ W\n",
    "        cost_prev = None\n",
    "        for i in range(self.max_iter):\n",
    "            y_pred = X @ W\n",
    "            gradient_W = 2./N * (X.T @ (y_pred - y)) + self.l_ratio * 2 * sum(W)  \n",
    "            W = W - self.alpha * gradient_W\n",
    "            cost = 1./N * sum((y_pred-y)**2) + self.l_ratio * sum(W**2)\n",
    "            if cost_prev and (abs(cost-cost_prev) < self.tol):\n",
    "                break\n",
    "            cost_prev = cost\n",
    "            \n",
    "        self.coefs = W\n",
    "   \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Метод для предсказаний линейной регрессии\n",
    "        X - матрица признаков\n",
    "        '''\n",
    "        \n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return X @ self.coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are amazing! Great work!\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression()\n",
    "my_reg.fit(X, y)\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки\n",
    "\n",
    "* с l1-регуляризацией (from sklearn.linear_model import Lasso, **первый и второй вариант**) или с l2-регуляризацией (from sklearn.linear_model import Ridge, **третий и четвертый вариант**)\n",
    "* со значением параметра регуляризации **0.1 - для первого и третьего варианта, 0.01 - для второго и четвертого варианта**. \n",
    "\n",
    "Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.38327530549976e-08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "reg5 = Ridge(alpha = 0.01)\n",
    "reg5.fit(X,y)\n",
    "print(mean_squared_error(y, reg5.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997.2369338516522\n"
     ]
    }
   ],
   "source": [
    "my_reg5 = LinearRegression(l_ratio = 0.01)\n",
    "my_reg5.fit(X,y)\n",
    "print(mean_squared_error(y, my_reg5.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Выводы:***\n",
    "Модель из sklearn имеет меньшую ошибку, чем наша модель. Это может быть связано с тем, что модель из sklearn написана лучше,чем наша. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 6* (1 балл).***\n",
    "Пусть $P, Q \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_Q tr(PQ)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 7* (1 балл).***\n",
    "Пусть $x, y \\in \\mathbb{R}^{n}, M \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_M x^T M y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решения заданий 6 и 7 можно написать на листочке и отправить в anytask вместе с заполненным ноутбуком."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
